{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"http://sct.inf.utfsm.cl/wp-content/uploads/2020/04/logo_di.png\" style=\"width:60%\">\n",
    "    <h1> INF285 - Computación Científica </h1>\n",
    "    <h2> Gradient Descent and Nonlinear Least-Square </h2>\n",
    "    <h2> <a href=\"#acknowledgements\"> [S]cientific [C]omputing [T]eam </a> </h2>\n",
    "    <h2> Version: 1.03</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='toc' />\n",
    "\n",
    "## Table of Contents\n",
    "* [Introduction](#intro)\n",
    "* [Gradient Descent](#GradientDescent) \n",
    "* [Gradient Descent in 1D](#GradientDescent1D)\n",
    "* [Gradient Descent for a 2D linear least-square problem](#GD_2D_LinearLeastSquare)\n",
    "* [Gradient Descent for a 2D nonlinear least-square problem](#GD_2D_NonLinearLeastSquare)\n",
    "* [Further Study](#FurtherStudy)\n",
    "* [Acknowledgements](#acknowledgements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as spla\n",
    "%matplotlib inline\n",
    "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets\n",
    "from sklearn import datasets\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, RadioButtons\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.size'] = 14\n",
    "mpl.rcParams['axes.labelsize'] = 20\n",
    "mpl.rcParams['xtick.labelsize'] = 14\n",
    "mpl.rcParams['ytick.labelsize'] = 14\n",
    "M=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='intro' />\n",
    "\n",
    "# Introduction\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "This jupyter notebook presents the algorithm of Gradient Descent applied to non-linear least-square problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='GradientDescent' />\n",
    "\n",
    "# Gradient Descent\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "The algorithm of Gradient Descent is used in Optimization, in particular, in problems when we want to minimize a function (or equivalently in maximization problem by changing the sign of the function).\n",
    "This algorithm considers a function $f(\\mathbf{x}):\\mathbb{R}^n \\rightarrow \\mathbb{R}$, which has at least a local minimum near the point $\\mathbf{x}_0$.\n",
    "The algorithm considers that we have access to the gradient of $f(\\mathbf{x})$, i.e. $\\nabla f(\\mathbf{x})$, which indicates the direction of fastest increase of $f(\\mathbf{x})$ at the point $\\mathbf{x}$, or equivalently, $-\\nabla f(\\mathbf{x})$ is teh direction of fastest decrease.\n",
    "Thus, the algorithm is the following,\n",
    "- Select an initial guess, say $\\mathbf{x}_0$\n",
    "- Compute the direction of fastest decrease: $\\mathbf{d}_0=-\\nabla f(\\mathbf{x}_0)$\n",
    "- Update the approximation $\\mathbf{x}_1=\\mathbf{x}_0+\\alpha\\,\\mathbf{d}_0$\n",
    "- Iterate until certain threshold is achieved.\n",
    "where $\\alpha$ is a scaling factor for the Gradient Descent step.\n",
    "The coefficient $\\alpha$ could also depend on on the iteration number, such that it adapts based on the iterations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='GradientDescent1D' />\n",
    "\n",
    "# Gradient Descent in 1D\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "To primary explain the algorithm, considere the following 1D example:\n",
    "$$\n",
    "f(x) = (x - 2)\\,\\sin(2\\,x) + x^2.\n",
    "$$\n",
    "We will first plot the function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining the function using a 'lambda' definition.\n",
    "f = lambda x: (x - 2)*np.sin(2*x) + np.power(x,2)\n",
    "\n",
    "# Defining the grid for plotting, the number '1000' indicates the number of points of the sample. \n",
    "# Suggestion: Change it and see what happends! For instance, what about if you change to 10?\n",
    "xx = np.linspace(-3,3,1000)\n",
    "# Plotting the function\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(xx,f(xx),'-',label=r'$f(x)$')\n",
    "plt.grid(True)\n",
    "plt.xlabel('$x$')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create an interactive use of the Gradient Descent in 1D where you could define the initial guess $x_0$, the scaling factor $\\alpha$ and the iteration number.\n",
    "In this numerical experiment we will the importance of the coefficient $\\alpha$, and how it is related to the 'gradient' and the initial guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a9ff937dc44bc59ee6c748d59a8af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.0, description='x0', max=3.0, min=-3.0), FloatSlider(value=1.0, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.GD_1D(x0=2, alpha=1, n=0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GD_1D(x0=2, alpha=1, n=0):\n",
    "    \n",
    "    # Defining the function using a 'lambda' definition and its derivative.\n",
    "    f = lambda x: (x-2)*np.sin(2*x)+np.power(x,2)\n",
    "    fp = lambda x: 2*x+2*(x-2)*np.cos(2*x)+np.sin(2*x)\n",
    "    \n",
    "    # Plotting the function and its derivative.\n",
    "    xx = np.linspace(-3,3,1000)\n",
    "    plt.figure(figsize=(14,7))\n",
    "    \n",
    "    ax = plt.subplot(1,2,1)\n",
    "    plt.plot(xx,f(xx),'b-',label=r'$f(x)$')\n",
    "    # Warning: The 'alpha' parameter for the plt.plot function corresponds to\n",
    "    # a transparency parameter, it is not related to the alpha parameter of\n",
    "    # the Gradient Descent explained before.\n",
    "    plt.plot(xx,fp(xx),'r-',label=r\"$f'(x)$\", alpha=0.5)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('$x$')\n",
    "    plt.title('Plot in linear scale')\n",
    "    \n",
    "    # Plotting outcome with no iterations\n",
    "    plt.plot(x0,f(x0),'k.',markersize=10,label=r'$x_i$')\n",
    "    plt.plot(x0,fp(x0),'m.',markersize=10,label=r\"$f'(x_i)$: 'Gradient'\")\n",
    "    ax = plt.subplot(1,2,2)\n",
    "    plt.semilogy(xx,np.abs(f(xx)),'b-',label=r\"$|f(x)|$\")\n",
    "    plt.semilogy(xx,np.abs(fp(xx)),'r-',label=r\"$|f'(x)|$\", alpha=0.5)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('$x$')\n",
    "    plt.title('Plot in logarithmic scale')\n",
    "    plt.semilogy(x0,np.abs(f(x0)),'k.',markersize=10,label=r'$x_i$')\n",
    "    plt.semilogy(x0,np.abs(fp(x0)),'m.',markersize=10,label=r\"$|f'(x_i)|$: 'Gradient'\")\n",
    "    \n",
    "    # Computing steps of Gradient Descent\n",
    "    if n>0:\n",
    "        xi_output=np.zeros(n+1)\n",
    "        xi_output[0]=x0\n",
    "        for k in range(n):\n",
    "            fp_x0=fp(x0)\n",
    "            x1 = x0-alpha*fp_x0\n",
    "            xi_output[k+1]=x1\n",
    "            x0 = x1\n",
    "        ax = plt.subplot(1,2,1)\n",
    "        plt.plot(xi_output,f(xi_output),'k.-',markersize=10,label=r'$x_i$')\n",
    "        plt.plot(xi_output,fp(xi_output),'m.',markersize=10)\n",
    "        ax = plt.subplot(1,2,2)\n",
    "        plt.semilogy(xi_output,np.abs(f(xi_output)),'k.-',markersize=10,label=r'$x_i$')\n",
    "        plt.semilogy(xi_output,np.abs(fp(xi_output)),'m.',markersize=10)\n",
    "        \n",
    "    \n",
    "    # Plotting outcome\n",
    "    ax = plt.subplot(1,2,1)\n",
    "    plt.legend(loc='best')\n",
    "    ax = plt.subplot(1,2,2)\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "interact(GD_1D,x0=(-3,3,0.1), alpha=(0,10,0.01), n=(0,100,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What conclusions could be draw?\n",
    "\n",
    "The main conclusion that can be draw is the importance of the selection of the parameter $\\alpha$ for the success of the task of finding a minimum of a function.\n",
    "Also, as ussual, the initial guess $x_0$ will help us to select different local minima.\n",
    "\n",
    "Question to think about:\n",
    "- What could happen if you normalize the 'gradient'? In 1D this would be computing the following coeficients: $GN=\\frac{f'(x_i)}{|f'(x_i)|}$, this will gives us the 'direction' where we should move (in 1D is just the sign of the derivative), then the coefficient $\\alpha$ may control a bit more the magnitude of each step from $x_i$ to $x_{i+1}$. So, how do we undertand this? Implement it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='GD_2D_LinearLeastSquare' />\n",
    "\n",
    "# Gradient Descent for a 2D linear least-square problem\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "In this case we will solve the following least-square problem:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\underbrace{\\begin{bmatrix}\n",
    "        1 & x_1 \\\\\n",
    "        1 & x_2 \\\\\n",
    "        1 & x_3 \\\\\n",
    "        \\vdots & \\vdots \\\\\n",
    "        1 & x_m\n",
    "    \\end{bmatrix}}_{\\displaystyle{A}}\n",
    "    \\underbrace{\\begin{bmatrix}\n",
    "        a\\\\\n",
    "        b\n",
    "    \\end{bmatrix}}_{\\mathbf{x}}\n",
    "    =\n",
    "    \\underbrace{\\begin{bmatrix}\n",
    "        y_1 \\\\\n",
    "        y_2 \\\\\n",
    "        y_3 \\\\\n",
    "        \\vdots\\\\\n",
    "        y_m \n",
    "    \\end{bmatrix}}_{\\displaystyle{\\mathbf{b}}}.\n",
    "\\end{equation}\n",
    "$$\n",
    "This overdetermined linear least-square problem can be translated to the following form:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    E(a,b)=\\left\\|\\mathbf{b}-A\\,\\mathbf{x}\\right\\|_2^2=\\sum_{i=1}^m (y_i-a-b\\,x_i)^2.\n",
    "\\end{equation}\n",
    "$$\n",
    "Now, to apply the Gradient Descent algorithm we need to compute the Gradient of $E(a,b)$ with respect to $a$ and $b$, which is the following,\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial E}{\\partial a} &= \\sum_{i=1}^m -2\\,(y_i-a-b\\,x_i),\\\\\n",
    "    \\frac{\\partial E}{\\partial b} &= \\sum_{i=1}^m -2\\,x_i\\,(y_i-a-b\\,x_i).\n",
    "\\end{align*}\n",
    "$$\n",
    "Notice that in this case we don't want to cancel out the \"-\" (minus) sign since it will change the direction of the Gradient.\n",
    "Now, we have everything to apply the Gradient Descent in 2D.\n",
    "For comparison purposes, we will also include the solution obtain by the normal equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36eebe0b54c42ef8cd7f67d1b2cfaa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.0, description='a0', max=4.0, min=-4.0), FloatSlider(value=2.0, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.GD_2D_linear(a0=2, b0=2, alpha=0.01, n=0, m=10)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GD_2D_linear(a0=2, b0=2, alpha=0.01, n=0, m=10):\n",
    "    \n",
    "    # Building data.\n",
    "    np.random.seed(0)\n",
    "    xi = np.random.normal(size=m)\n",
    "    yi = -2+xi+np.random.normal(loc=0, scale=0.5, size=m)\n",
    "\n",
    "    # Defining matrix A and the right-hand-side.\n",
    "    # Recall that we usually denote as b the right-hand-side but to avoid confusion with\n",
    "    # the coefficient b, we will just call it RHS.\n",
    "    A = np.ones((m,2))\n",
    "    A[:,1] = xi\n",
    "    RHS = yi\n",
    "    \n",
    "    # Defining the Gradient\n",
    "    E = lambda a, b: np.sum(np.power(yi-a-b*xi,2))\n",
    "    G = lambda a, b: np.array([np.sum(-2*(yi-a-b*xi)), np.sum(-2*xi*(yi-a-b*xi))],dtype=float)\n",
    "    # This fucntion will help us to evaluate the Gradient on the points (X[i,j],Y[i,j])\n",
    "    def E_mG_XY(AA,BB):\n",
    "        Z = np.zeros_like(AA)\n",
    "        U = np.zeros_like(AA)\n",
    "        V = np.zeros_like(AA)\n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                Z[i,j]=E(AA[i,j],BB[i,j])\n",
    "                uv = -G(AA[i,j],BB[i,j])\n",
    "                U[i,j] = uv[0]\n",
    "                V[i,j] = uv[1]\n",
    "        return Z, U, V\n",
    "    \n",
    "    # Plotting the function and its gradient.\n",
    "    # Credits: \n",
    "    #  https://matplotlib.org/stable/gallery/images_contours_and_fields/plot_streamplot.html\n",
    "    #  https://scipython.com/blog/visualizing-a-vector-field-with-matplotlib/\n",
    "    x = np.linspace(-5,5,m)\n",
    "    AA, BB = np.meshgrid(x,x)\n",
    "    fig = plt.figure(figsize=(14,10))\n",
    "    Z, U, V = E_mG_XY(AA,BB)\n",
    "    cont = plt.contour(AA,BB,Z, 100)\n",
    "    stream = plt.streamplot(AA, BB, U, V, color=Z, linewidth=2, cmap='autumn', arrowstyle='->', arrowsize=2)\n",
    "    fig.colorbar(stream.lines)\n",
    "    fig.colorbar(cont)\n",
    "    plt.scatter(a0, b0, s=300, marker='.', c='k')\n",
    "    my_grad = G(a0,b0)\n",
    "    my_title = r'$\\alpha=$ %.4f, $E(a,b)=$ %.4f, $\\nabla E(a,b)=$ [%.4f, %.4f]' %  (alpha, E(a0,b0), my_grad[0], my_grad[1])\n",
    "    plt.title(my_title)\n",
    "    \n",
    "    # Computing steps of Gradient Descent\n",
    "    if n>0:\n",
    "        ab_output=np.zeros((n+1,2))\n",
    "        z0 = np.array([a0,b0],dtype=float)\n",
    "        z0[0] = a0\n",
    "        z0[1] = b0\n",
    "        ab_output[0,:]=z0\n",
    "        # The Gradient Descent Algorithm\n",
    "        for k in range(n):\n",
    "            G_E_0=G(z0[0],z0[1])\n",
    "            z1 = z0-alpha*G_E_0\n",
    "            ab_output[k+1,:]=z1\n",
    "            z0 = z1\n",
    "            plt.scatter(z1[0], z1[1], s=300, marker='.', c='k')\n",
    "        plt.plot(ab_output[:,0],ab_output[:,1],'k-')\n",
    "        my_grad = G(ab_output[-1,0],ab_output[-1,1])\n",
    "        my_title = r'$\\alpha=$ %.4f, $E(a,b)=$ %.4f, $\\nabla E(a,b)=$ [%.4f, %.4f]' %  (alpha, E(ab_output[-1,0],ab_output[-1,1]), my_grad[0], my_grad[1])\n",
    "        plt.title(my_title)\n",
    "    plt.show()\n",
    "    \n",
    "interact(GD_2D_linear, a0=(-4,4,0.1), b0=(-4,4,0.1), alpha=(0,0.1,0.0001), n=(0,100,1), m=(10,100,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous implementation we used the following notation:\n",
    "- $n$: Number of iteration of Gradient Descent\n",
    "- Black dot: Solution $[a_n,b_n]$ at $n$-th step of the Gradient Descent.\n",
    "- Red-Yellow streamplot: Stream plot of the vector field generated by minus the Gradient of the error function $E(a,b)$\n",
    "- Blue-Green contour: Contour plot of the error function $E(a,b)$.\n",
    "\n",
    "Questions:\n",
    "- Try: $\\alpha=0.02$, $n=20$, and $m=10$. What do you observe? (keep initialization values of $a_0$ and $a_1$)\n",
    "- Try: $\\alpha=0.04$, $n=20$, and $m=10$. What do you observe? (keep initialization values of $a_0$ and $a_1$)\n",
    "- Try: $\\alpha=0.08$, $n=20$, and $m=10$. What do you observe? (keep initialization values of $a_0$ and $a_1$)\n",
    "- Can we use a large value of $\\alpha$?\n",
    "- How is related $\\alpha$ and the iteration number $n$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='GD_2D_NonLinearLeastSquare' />\n",
    "\n",
    "# Gradient Descent for a 2D nonlinear least-square problem\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "In this case, we will explore the use of the the Gradient Descent algorithm applied to a nonlinear least-square problem with an exponential fit.\n",
    "Let the function to be fit be,\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y(t) = a\\,\\exp(b\\,t),\n",
    "\\end{equation}\n",
    "$$\n",
    "where the error function is defined as follows,\n",
    "$$\n",
    "\\begin{equation}\n",
    "    E(a,b)=\\sum_{i=1}^m (y_i-a\\,\\exp(b\\,t_i))^2.\n",
    "\\end{equation}\n",
    "$$\n",
    "Now, to apply the Gradient Descent algorithm we need to compute the Gradient of $E(a,b)$ with respect to $a$ and $b$, which is the following,\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial E}{\\partial a} &= \\sum_{i=1}^m 2\\,\\exp(b\\,t_i)(a\\,\\exp(b\\,t_i)-y_i),\\\\\n",
    "    \\frac{\\partial E}{\\partial b} &= \\sum_{i=1}^m 2\\,a\\,\\exp(b\\,t_i)\\,t_i\\,(a\\,\\exp(b\\,t_i)-y_i).\n",
    "\\end{align*}\n",
    "$$\n",
    "As you may expect, this approach may create very large values for the gradient, which will be very challenging to handle them numerically.\n",
    "So, an alternative approach is the following,  which we will call it \"The Variant\":\n",
    "- Select an initial guess, say $\\mathbf{x}_0$\n",
    "- Compute the direction of fastest decrease: $\\mathbf{d}_0=-\\nabla E(\\mathbf{x}_0)$\n",
    "- Update the approximation $\\mathbf{x}_1=\\mathbf{x}_0+\\alpha\\,\\frac{\\mathbf{d}_0}{\\|\\mathbf{d}_0\\|}$\n",
    "- Iterate until certain threshold is achieved.\n",
    "Thus, the only change is on the magnitud of the **direction** vector used.\n",
    "In this case, it will be a unitary direction. \n",
    "This brings the advantage that $\\alpha$ now controls the **length** of the update.\n",
    "This is useful when you want to control the increment, otherwise it may require a very fine tuning of the parameter (or in general hyperparameter tuning!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143bbb79e5bb4688b4ea5fd439250f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.75, description='a0', max=2.0, min=-2.0, step=0.01), FloatSlider(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.GD_2D_nonlinear(a0=0.75, b0=0.75, alpha=0, n=0, m=10, TheVariantFlag=False)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GD_2D_nonlinear(a0=0.75, b0=0.75, alpha=0, n=0, m=10, TheVariantFlag=False):\n",
    "    \n",
    "    # Building data.\n",
    "    np.random.seed(0)\n",
    "    a = 1.1\n",
    "    b = 0.23\n",
    "    y = lambda t: a*np.exp(b*t)\n",
    "    T = 10\n",
    "    ti = T*(np.random.rand(m)*2-1)\n",
    "    yi = y(ti)+np.random.normal(loc=0, scale=0.1, size=m)\n",
    "    \n",
    "    # Defining the Gradient\n",
    "    E = lambda a, b: np.sum(np.power(yi-a*np.exp(b*ti),2))\n",
    "    G = lambda a, b: np.array([np.sum(2*np.exp(b*ti)*(a*np.exp(b*ti)-yi)), np.sum(2*a*np.exp(b*ti)*ti*(a*np.exp(b*ti)-yi))],dtype=float)\n",
    "    # This fucntion will help us to evaluate the Gradient on the points (X[i,j],Y[i,j])\n",
    "    def E_mG_XY(AA,BB):\n",
    "        Z = np.zeros_like(AA)\n",
    "        U = np.zeros_like(AA)\n",
    "        V = np.zeros_like(AA)\n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                Z[i,j]=E(AA[i,j],BB[i,j])\n",
    "                uv = -G(AA[i,j],BB[i,j])\n",
    "                U[i,j] = uv[0]\n",
    "                V[i,j] = uv[1]\n",
    "        return Z, U, V\n",
    "    \n",
    "    # Plotting the function and its gradient.\n",
    "    # Credits: \n",
    "    #  https://matplotlib.org/stable/gallery/images_contours_and_fields/plot_streamplot.html\n",
    "    #  https://scipython.com/blog/visualizing-a-vector-field-with-matplotlib/\n",
    "    x = np.linspace(-3,3,m)\n",
    "    AA, BB = np.meshgrid(x,x)\n",
    "    fig = plt.figure(figsize=(14,10))\n",
    "    Z, U, V = E_mG_XY(AA,BB)\n",
    "    cont = plt.contour(AA,BB,Z, 10)\n",
    "    stream = plt.streamplot(AA, BB, U, V, color=Z, linewidth=2, cmap='autumn', arrowstyle='->', arrowsize=2)\n",
    "    fig.colorbar(stream.lines)\n",
    "    fig.colorbar(cont)\n",
    "    plt.scatter(a0, b0, s=300, marker='.', c='k')\n",
    "    my_grad = G(a0,b0)\n",
    "    my_title = r'$\\alpha=$ %.4f, $E(a,b)=$ %.4f, $\\nabla E(a,b)=$ [%.4f, %.4f]' %  (alpha, E(a0,b0), my_grad[0], my_grad[1])\n",
    "    plt.title(my_title)\n",
    "    \n",
    "    # Computing steps of Gradient Descent\n",
    "    if n>0:\n",
    "        ab_output=np.zeros((n+1,2))\n",
    "        z0 = np.array([a0,b0],dtype=float)\n",
    "        z0[0] = a0\n",
    "        z0[1] = b0\n",
    "        ab_output[0,:]=z0\n",
    "        # The Gradient Descent Algorithm\n",
    "        for k in range(n):\n",
    "            G_E_0=G(z0[0],z0[1])\n",
    "            if not TheVariantFlag:\n",
    "                # Traditional GD\n",
    "                z1 = z0-alpha*G_E_0\n",
    "            else:\n",
    "                # The Variant! Why would this be useful?\n",
    "                z1 = z0-alpha*G_E_0/np.linalg.norm(G_E_0)\n",
    "            ab_output[k+1,:]=z1\n",
    "            z0 = z1\n",
    "            plt.scatter(z1[0], z1[1], s=300, marker='.', c='k')\n",
    "        plt.plot(ab_output[:,0],ab_output[:,1],'k-')\n",
    "        my_grad = G(ab_output[-1,0],ab_output[-1,1])\n",
    "        my_title = r'$\\alpha=$ %.6f, $E(a,b)=$ %.4f, $\\nabla E(a,b)=$ [%.4f, %.4f]' %  (alpha, E(ab_output[-1,0],ab_output[-1,1]), my_grad[0], my_grad[1])\n",
    "        plt.title(my_title)\n",
    "        print('GD found:',ab_output[-1,0],ab_output[-1,1])\n",
    "        \n",
    "    # Plotting the original data and the \"transformed\" solution\n",
    "    # Using the same notation from classnotes:\n",
    "    A = np.ones((m,2))\n",
    "    A[:,1]=ti\n",
    "    K_c2 =np.linalg.lstsq(A,np.log(yi), rcond=None)[0]\n",
    "    c1_ls = np.exp(K_c2[0])\n",
    "    c2_ls = K_c2[1]\n",
    "    print('Transformed Linear LS solution:',c1_ls, c2_ls)\n",
    "    plt.plot(c1_ls,c2_ls,'ms',markersize=20, label='Transformed Linear LS')\n",
    "    print('Original data:',a,b)\n",
    "    plt.plot(a,b,'bd',markersize=20, label='Original data')\n",
    "        \n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "radio_button_TheVariant=RadioButtons(\n",
    "    options=[('Traditional GD',False),('The Variant GD',True)],\n",
    "    value=False,\n",
    "    description='GD type:',\n",
    "    disabled=False\n",
    ")\n",
    "    \n",
    "interact(GD_2D_nonlinear, a0=(-2,2,0.01), b0=(-2,2,0.01), alpha=(0,1,0.0001), n=(0,1000,1), m=(10,100,10), TheVariantFlag=radio_button_TheVariant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous implementation we used the following notation:\n",
    "- $n$: Number of iteration of Gradient Descent\n",
    "- Black dot: Solution $[a_n,b_n]$ at $n$-th step of the Gradient Descent.\n",
    "- Red-Yellow streamplot: Stream plot of the vector field generated by minus the Gradient of the error function $E(a,b)$\n",
    "- Blue-Green contour: Contour plot of the error function $E(a,b)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='FurtherStudy' />\n",
    "\n",
    "# Further Study\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "Another extension of the Gradient Descent is the so called _Stochastic Gradient Descent Method (SGD)_, very popular in Data Science, Machine Learning and Artificial Neural Networks (ANN) in general.\n",
    "Here a interesting reference: [Link](https://optimization.cbe.cornell.edu/index.php?title=Stochastic_gradient_descent), another good reference is the textbook _[Linear Algebra and learning from data](https://math.mit.edu/~gs/learningfromdata/)_ by Professor Gilbert Strang, page 359.\n",
    "\n",
    "A simple way to undertand the SGD is as follows:\n",
    "- Select an initial guess, say $\\mathbf{x}_0$\n",
    "- Select a sample of data $D_k$ from the dataset $D$, where $k$ indicates the number of _data points_ of the sample.\n",
    "- Define the error only including the _data points_ from the sample $D_k$, and call it $E_k(\\cdot)$\n",
    "- Compute the direction of fastest decrease: $\\mathbf{d}^{[k]}_0=-\\nabla E_k(\\mathbf{x}_0)$\n",
    "- Update the approximation $\\mathbf{x}_1=\\mathbf{x}_0+\\alpha\\,\\mathbf{d}^{[k]}_0$\n",
    "- Iterate until certain threshold is achieved.\n",
    "\n",
    "So, the key point here is that we don't use all the dataset $D$ to update the coefficients on each iteration, it clearly has the advantage that the computation is way faster but the question it arises is that, _would this affect the convergence?_ Answer: Try it numerically! In general, this approximation behaves very well when used in ANN since in ANN they don't want to _overfit_ the coefficients to the dataset.\n",
    "Notice that the size of the sample $k$ could it be even $1$, this makes the computation very fast!\n",
    "\n",
    "Before we finish, it is useful to make the connection between the terminology used here and the terminology used in ANN,\n",
    "- Error function $E(\\cdot)$ $\\rightarrow$ Loss function $L(\\cdot)=\\frac{1}{m}E(\\cdot)$. Notice however that the loss function $L(\\cdot)$ in ANN may not have a quadratic form, for instance it could be $\\frac{1}{m}\\sum |y_i-a-b\\,x_i|$, i.e. the sum of the absolutes values. And in general it may also consider _activator functions_ $\\phi(\\cdot)$ to model neurons, which modify the loss function as follows $\\frac{1}{m}\\sum \\phi(y_i-a-b\\,x_i)$.\n",
    "- Coefficient $\\alpha$ $\\rightarrow$ It is called _learning rate_ in ANN, since it controls how fast the ANN _learns_ from samples. As we say in this jupyter notebook, it is very important for a good _training_.\n",
    "- Adjusting coefficients $\\rightarrow$ Training. This the step where the ANN _learn_ from _samples_. Notice that in ANN it may not be required a low error, since it may affect the _generalization capabilities_ of the ANN.\n",
    "- A brief but useful explanation of Deep Learning is [here](https://math.mit.edu/%7Egs/learningfromdata/siam.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='acknowledgements' />\n",
    "\n",
    "# Acknowledgements\n",
    "[Back to TOC](#toc)\n",
    "* _Material created by professor Claudio Torres_ (`ctorres@inf.utfsm.cl`) DI UTFSM. November 2021.- v1.0.\n",
    "* _Update November 2021 - v1.01 - C.Torres_ : Fixing TOC.\n",
    "* _Update November 2021 - v1.02 - C.Torres_ : Fixing titles size, typos and adding further study section.\n",
    "* _Update June 2022 - v1.03 - C.Torres_ : Fixing typo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  },
  "widgets": {
   "state": {
    "7fd91d6f0d2545e7af10aae93cfe07e9": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
